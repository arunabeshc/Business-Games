{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58eab5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import gradio as gr\n",
    "from io import StringIO\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344d3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Embed a single query text.\"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96398cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 17\n",
      "Document types found: {'Process_Doc'}\n"
     ]
    }
   ],
   "source": [
    "def load_docx_content(file_path):\n",
    "    \"\"\"Load content from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        content = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                content.append(paragraph.text.strip())\n",
    "        return '\\n'.join(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Simple text splitter.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - chunk_overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Load all DOCX files from the Documents folder\n",
    "files = glob.glob(\"Documents/*.docx\")\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "for file_path in files:\n",
    "    doc_type = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    content = load_docx_content(file_path)\n",
    "    \n",
    "    if content:\n",
    "        chunks = split_text(content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            all_metadatas.append({\n",
    "                \"doc_type\": doc_type,\n",
    "                \"file_path\": file_path,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "            all_ids.append(f\"{doc_type}_{i}_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "print(f\"Total number of chunks: {len(all_chunks)}\")\n",
    "print(f\"Document types found: {set(meta['doc_type'] for meta in all_metadatas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23577c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"agile_process\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6016d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection didn't exist or couldn't be deleted: Collection [process_docs] does not exists\n",
      "Added batch 1/1\n",
      "Vectorstore created with 17 documents\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to create the vectorstore again\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "collection_name = \"process_docs\"\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(\"Existing collection deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection didn't exist or couldn't be deleted: {e}\")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Add documents to the collection in batches (ChromaDB has limits)\n",
    "batch_size = 100\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    batch_chunks = all_chunks[i:i + batch_size]\n",
    "    batch_metadatas = all_metadatas[i:i + batch_size]\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    \n",
    "    # Generate embeddings for this batch\n",
    "    batch_embeddings = embeddings_model.embed_documents(batch_chunks)\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=batch_chunks,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_chunks) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "# Test query (uncomment to test)\n",
    "# results = query_documents(\"your query here\")\n",
    "# print(\"Query results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569eaf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

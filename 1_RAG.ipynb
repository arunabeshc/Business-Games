{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c029e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf83ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from docx import Document\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import plotly.graph_objects as go\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccbd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"agile_process\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8345ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58e04c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 17\n",
      "Document types found: {'Process_Doc'}\n"
     ]
    }
   ],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Embed a single query text.\"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()\n",
    "\n",
    "def load_docx_content(file_path):\n",
    "    \"\"\"Load content from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        content = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                content.append(paragraph.text.strip())\n",
    "        return '\\n'.join(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Simple text splitter.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - chunk_overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Load all DOCX files from the Documents folder\n",
    "files = glob.glob(\"Documents/*.docx\")\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "for file_path in files:\n",
    "    doc_type = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    content = load_docx_content(file_path)\n",
    "    \n",
    "    if content:\n",
    "        chunks = split_text(content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            all_metadatas.append({\n",
    "                \"doc_type\": doc_type,\n",
    "                \"file_path\": file_path,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "            all_ids.append(f\"{doc_type}_{i}_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "print(f\"Total number of chunks: {len(all_chunks)}\")\n",
    "print(f\"Document types found: {set(meta['doc_type'] for meta in all_metadatas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982bbc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection didn't exist or couldn't be deleted: Collection [process_docs] does not exists\n",
      "Added batch 1/1\n",
      "Vectorstore created with 17 documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"agile_process\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "collection_name = \"process_docs\"\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(\"Existing collection deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection didn't exist or couldn't be deleted: {e}\")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Add documents to the collection in batches (ChromaDB has limits)\n",
    "batch_size = 100\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    batch_chunks = all_chunks[i:i + batch_size]\n",
    "    batch_metadatas = all_metadatas[i:i + batch_size]\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    \n",
    "    # Generate embeddings for this batch\n",
    "    batch_embeddings = embeddings_model.embed_documents(batch_chunks)\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=batch_chunks,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_chunks) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "# Example: Query the collection\n",
    "def query_documents(query_text, n_results=5):\n",
    "    \"\"\"Query the document collection.\"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query_text)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test query (uncomment to test)\n",
    "# results = query_documents(\"your query here\")\n",
    "# print(\"Query results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2afa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "\n",
    "class MultiComponentRAG:\n",
    "    def __init__(self, db_path: str = \"./chroma_db\", collection_name: str = \"process_docs\"):\n",
    "        # Initialize ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        self.collection = self.client.get_collection(name=collection_name)\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize OpenAI (replace with your preferred LLM)\n",
    "        # openai.api_key = \"your-api-key-here\"\n",
    "        \n",
    "        # Cache for storing retrieved contexts\n",
    "        self.context_cache = {}\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, n_results: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant document chunks with metadata.\"\"\"\n",
    "        query_embedding = self.embeddings_model.encode([query]).tolist()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"documents\": results[\"documents\"][0],\n",
    "            \"metadatas\": results[\"metadatas\"][0],\n",
    "            \"distances\": results[\"distances\"][0]\n",
    "        }\n",
    "    \n",
    "    def get_multi_component_context(self, query: str, max_chunks: int = 15) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve and organize multiple document components for complex queries.\n",
    "        \"\"\"\n",
    "        # Get initial relevant chunks\n",
    "        results = self.retrieve_relevant_chunks(query, n_results=max_chunks)\n",
    "        \n",
    "        # Group chunks by document type\n",
    "        doc_groups = {}\n",
    "        for doc, metadata, distance in zip(results[\"documents\"], results[\"metadatas\"], results[\"distances\"]):\n",
    "            doc_type = metadata.get(\"doc_type\", \"unknown\")\n",
    "            if doc_type not in doc_groups:\n",
    "                doc_groups[doc_type] = []\n",
    "            doc_groups[doc_type].append({\n",
    "                \"content\": doc,\n",
    "                \"metadata\": metadata,\n",
    "                \"relevance_score\": 1 - distance  # Convert distance to similarity\n",
    "            })\n",
    "        \n",
    "        # Build comprehensive context\n",
    "        context_parts = []\n",
    "        context_parts.append(\"=== RELEVANT DOCUMENT SECTIONS ===\\n\")\n",
    "        \n",
    "        for doc_type, chunks in doc_groups.items():\n",
    "            context_parts.append(f\"\\n--- FROM DOCUMENT: {doc_type.upper()} ---\")\n",
    "            \n",
    "            # Sort chunks by relevance within each document\n",
    "            chunks.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks[:5]):  # Limit chunks per document\n",
    "                context_parts.append(f\"\\n[Chunk {i+1} - Relevance: {chunk['relevance_score']:.3f}]\")\n",
    "                context_parts.append(chunk[\"content\"])\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def extract_key_concepts(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract key concepts from query to perform multi-faceted retrieval.\"\"\"\n",
    "        # Simple keyword extraction (you could use more sophisticated NLP here)\n",
    "        # Remove common words and extract key terms\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'what', 'how', 'when', 'where', 'why', 'who'}\n",
    "        \n",
    "        words = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "        key_concepts = [word for word in words if len(word) > 3 and word not in stop_words]\n",
    "        \n",
    "        return key_concepts[:5]  # Return top 5 concepts\n",
    "    \n",
    "    def enhanced_retrieval(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced retrieval that looks for multiple components and related concepts.\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        \n",
    "        # 1. Direct query retrieval\n",
    "        direct_context = self.get_multi_component_context(query, max_chunks=10)\n",
    "        contexts.append((\"Direct Query\", direct_context))\n",
    "        \n",
    "        # 2. Key concept based retrieval\n",
    "        key_concepts = self.extract_key_concepts(query)\n",
    "        for concept in key_concepts:\n",
    "            concept_context = self.get_multi_component_context(concept, max_chunks=5)\n",
    "            contexts.append((f\"Concept: {concept}\", concept_context))\n",
    "        \n",
    "        # Combine all contexts\n",
    "        final_context = \"\\n\\n\" + \"=\"*50 + \"\\n\"\n",
    "        final_context += \"COMPREHENSIVE DOCUMENT CONTEXT\\n\"\n",
    "        final_context += \"=\"*50 + \"\\n\\n\"\n",
    "        \n",
    "        for context_type, context_content in contexts:\n",
    "            final_context += f\"\\n{'='*20} {context_type} {'='*20}\\n\"\n",
    "            final_context += context_content\n",
    "            final_context += \"\\n\"\n",
    "        \n",
    "        return final_context\n",
    "    \n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate response using LLM with retrieved context.\"\"\"\n",
    "        \n",
    "        # Create a comprehensive prompt\n",
    "        prompt = f\"\"\"You are an AI assistant that answers questions based on document content. \n",
    "        \n",
    "Use the provided document context to answer the user's question comprehensively. \n",
    "If the question requires information from multiple parts of the documents, make sure to reference and synthesize information from different sections.\n",
    "\n",
    "DOCUMENT CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer based primarily on the provided document context\n",
    "2. If information spans multiple documents or sections, synthesize them clearly\n",
    "3. Cite which document types or sections you're referencing\n",
    "4. If the context doesn't contain sufficient information, say so clearly\n",
    "5. Provide a comprehensive answer that addresses all aspects of the question\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        # For demo purposes, using a simple response\n",
    "        # Replace this with actual LLM API call\n",
    "        try:\n",
    "            # Example with OpenAI (uncomment and configure)\n",
    "            # response = openai.ChatCompletion.create(\n",
    "            #     model=\"gpt-3.5-turbo\",\n",
    "            #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            #     max_tokens=1000,\n",
    "            #     temperature=0.7\n",
    "            # )\n",
    "            # return response.choices[0].message.content\n",
    "            \n",
    "            # Placeholder response for demo\n",
    "            return f\"\"\"Based on the retrieved document context, I can see information from multiple document sections:\n",
    "\n",
    "**Key Points Found:**\n",
    "- Document types referenced: {', '.join(set(re.findall(r'FROM DOCUMENT: (\\w+)', context)))}\n",
    "- Multiple relevant sections were found across different documents\n",
    "- The query appears to require synthesis of information from various components\n",
    "\n",
    "**Answer:** \n",
    "[This is where the actual LLM response would appear. The system has retrieved context from multiple document components as requested.]\n",
    "\n",
    "**Sources Referenced:**\n",
    "{context[:200]}... (truncated for display)\n",
    "\n",
    "*Note: Replace this placeholder with actual LLM integration*\"\"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat_interface(self, message: str, history: List[List[str]]) -> str:\n",
    "        \"\"\"Main chat interface for Gradio.\"\"\"\n",
    "        try:\n",
    "            # Enhanced retrieval for multi-component queries\n",
    "            context = self.enhanced_retrieval(message)\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.generate_response(message, context)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = MultiComponentRAG()\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Multi-Component Document RAG System\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # 🤖 Multi-Component Document RAG System\n",
    "        \n",
    "        Ask questions that may require referencing multiple parts of your document database.\n",
    "        The system will intelligently retrieve and synthesize information from various document components.\n",
    "        \n",
    "        **Examples of complex queries:**\n",
    "        - \"Compare the methodology sections across different documents\"\n",
    "        - \"What are the common themes mentioned in all documents?\"\n",
    "        - \"Summarize the key findings from multiple research papers\"\n",
    "        \"\"\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(\n",
    "            height=500,\n",
    "            show_label=False,\n",
    "            container=True,\n",
    "            show_copy_button=True\n",
    "        )\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Ask a question that might require multiple document components...\",\n",
    "            label=\"Your Question\",\n",
    "            lines=2\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "        \n",
    "        # Advanced options\n",
    "        with gr.Accordion(\"Advanced Options\", open=False):\n",
    "            max_chunks = gr.Slider(\n",
    "                minimum=5,\n",
    "                maximum=25,\n",
    "                value=15,\n",
    "                step=1,\n",
    "                label=\"Maximum chunks to retrieve\",\n",
    "                info=\"Higher values provide more context but may be slower\"\n",
    "            )\n",
    "            \n",
    "            show_context = gr.Checkbox(\n",
    "                label=\"Show retrieved context\",\n",
    "                value=False,\n",
    "                info=\"Display the document context used for generating the response\"\n",
    "            )\n",
    "        \n",
    "        def respond(message, history, max_chunks_val, show_context_val):\n",
    "            # Temporarily update max_chunks in the system\n",
    "            original_method = rag_system.get_multi_component_context\n",
    "            \n",
    "            def modified_method(query, max_chunks=max_chunks_val):\n",
    "                return original_method(query, max_chunks)\n",
    "            \n",
    "            rag_system.get_multi_component_context = modified_method\n",
    "            \n",
    "            # Get response\n",
    "            bot_message = rag_system.chat_interface(message, history)\n",
    "            \n",
    "            # Add context if requested\n",
    "            if show_context_val:\n",
    "                context = rag_system.enhanced_retrieval(message)\n",
    "                bot_message += f\"\\n\\n--- RETRIEVED CONTEXT ---\\n{context[:1000]}...\"\n",
    "            \n",
    "            history.append([message, bot_message])\n",
    "            return history, \"\"\n",
    "        \n",
    "        def clear_chat():\n",
    "            return [], \"\"\n",
    "        \n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, max_chunks, show_context],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, max_chunks, show_context],\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(clear_chat, outputs=[chatbot, msg])\n",
    "        \n",
    "        # Example queries\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                \"What are the main topics covered across all documents?\",\n",
    "                \"Compare the methodologies mentioned in different documents\",\n",
    "                \"Summarize the key findings from the research papers\",\n",
    "                \"What recommendations are made in the policy documents?\",\n",
    "                \"Are there any contradictions between different document sections?\"\n",
    "            ],\n",
    "            inputs=msg\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        share=False,  # Set to True if you want a public link\n",
    "        debug=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf83ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import gradio as gr\n",
    "from io import StringIO\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"agile_process\"\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58486cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Embed a single query text.\"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"agile_process\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n",
    "collection_name = \"process_docs\"\n",
    "collection = client.get_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query the collection\n",
    "def query_documents(query_text, n_results=10):\n",
    "    \"\"\"Query the document collection.\"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query_text)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLLogFormatter(logging.Formatter):\n",
    "    AGENT_COLORS = {\n",
    "        \"FrontierAgent\": \"blue\",\n",
    "        \"Default\": \"black\"\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        record.asctime = self.formatTime(record, self.datefmt)\n",
    "        logger_name = record.name.split('.')[-1]  # often \"__main__\" or module\n",
    "        color = self.AGENT_COLORS.get(logger_name, self.AGENT_COLORS[\"Default\"])\n",
    "        return f'<div style=\"color:{color}\">[{record.asctime}] [{logger_name}] [{record.levelname}] {record.getMessage()}</div>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4812ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_stream = StringIO()\n",
    "html_log_stream = StringIO()\n",
    "\n",
    "def init_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    \n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Memory buffer handler\n",
    "    stream_handler = logging.StreamHandler(log_stream)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s] [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "     # HTML handler for Gradio\n",
    "    html_stream_handler = logging.StreamHandler(html_log_stream)\n",
    "    html_stream_handler.setLevel(logging.INFO)\n",
    "    html_stream_handler.setFormatter(HTMLLogFormatter(\n",
    "        fmt=\"%(asctime)s [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\"\n",
    "    ))\n",
    "    root.addHandler(html_stream_handler)\n",
    "\n",
    "    root.addHandler(handler)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e79d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_logs():\n",
    "    html_log_stream.seek(0)\n",
    "    return html_log_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb344bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_logs():\n",
    "    log_stream.truncate(0)\n",
    "    log_stream.seek(0)\n",
    "    html_log_stream.truncate(0)\n",
    "    html_log_stream.seek(0)\n",
    "    return None, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e281990",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You devise the strategy on how to set up and start a new data engineering project from scratch. You ask the user\n",
    "    to share details about source systems, the number of final data marts, and a brief outline of the overall architecture. Do not proceed \n",
    "    unless the user shares the information related to overall architecture, details about source systems, and the number of target data\n",
    "    marts. On getting the aforesaid information from the user, you will make the tool call using the return_context_function and strictly\n",
    "    adhere to the agile process details that the tool call returns. You will eventually display the epics, features, and user stories \n",
    "    needed to achieve the end objective, which is to create the final data marts. Always provide epic, feature, and story details and \n",
    "    include points for each story along with the output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bbedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_context_function = {\n",
    "    \"name\": \"return_context\",\n",
    "    \"description\": \"\"\"Call this tool after the user confirms the details about source systems, the number of target data marts, and the \n",
    "    overall architecture. For any details related to agile processes, you will strictly adhere to the output that you gather from this \n",
    "    tool call.\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"details_source_systems\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The details about source systems\",\n",
    "            },\n",
    "            \"overall_architecture\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the architecture\",\n",
    "            },\n",
    "            \"target_data_marts\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the target data marts\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"details_source_systems\",\"overall_architecture\",\"target_data_marts\"],\n",
    "    \"additionalProperties\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff50278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": return_context_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceaf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(name, args):\n",
    "    source = args.get('details_source_systems')\n",
    "    architecture = args.get('overall_architecture')\n",
    "    marts = args.get('target_data_marts')\n",
    "    if name.replace('\"','') == \"return_context\":\n",
    "        context=return_context(collection, f\"Source details -\\n{source}\\n\\nArchitecture Details -\\n{architecture}\\n\\nMart Details -\\n{marts}\")\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_context(collection, user_query):\n",
    "    context = \"\\n\\nProviding some context from relevant information -\\n\\n\"\n",
    "    retrieved = collection.query(\n",
    "        query_embeddings=[embeddings_model.embed_query(user_query)],\n",
    "        n_results=10,  # e.g., 5 or 10\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    retrieved_chunks = retrieved[\"documents\"][0]\n",
    "    context+= \"\\n\\n\".join(retrieved_chunks)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602012dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_open_ai(history):\n",
    "    openai=OpenAI()\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history \n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages, tools=tools)\n",
    "\n",
    "    tool_responses = []\n",
    "\n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_calls = message.tool_calls  # renamed to avoid UnboundLocalError\n",
    "\n",
    "        print(f\"tool calls \\n\\n {tool_calls}\")\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_id = tool_call.id\n",
    "            name = tool_call.function.name\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            # Call the tool handler\n",
    "            result = \"\"\n",
    "            if name == \"return_context\":\n",
    "                result = handle_tool_call(name, args)\n",
    "\n",
    "            tool_responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_id,\n",
    "                \"content\": json.dumps(result),\n",
    "            })\n",
    "\n",
    "        print(f\"tool responses {tool_responses}\")\n",
    "        messages.append(message)\n",
    "        messages.extend(tool_responses)  # important fix here\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history,Model):\n",
    "    if Model==\"Open AI\":\n",
    "        openai = OpenAI()\n",
    "        history = chat_open_ai(history)\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ac729",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"ðŸ‘‹ Hello! How can I assist you today? If you're looking to set up a new data engineering project, please provide me with details about the source systems, the number of target data marts, and a brief outline of the overall architecture.\"\"\"\n",
    "\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    #log_box {\n",
    "        height: 300px;\n",
    "        overflow-y: scroll;\n",
    "        border: 1px solid #ccc;\n",
    "        padding: 10px;\n",
    "        background: #f9f9f9;\n",
    "        font-family: monospace;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\"\"\") as ui:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=300, type=\"messages\")\n",
    "        with gr.Column(scale=1):\n",
    "            \n",
    "            logs_box = gr.HTML(label=\"Logs\", elem_id=\"log_box\")\n",
    "    with gr.Row():\n",
    "        Model = gr.Dropdown([\"Open AI\",\"XX\"],\n",
    "                              # value=[\"Open AI\",\"Claude\"],\n",
    "                              multiselect=False,\n",
    "                              label=\"Model\",\n",
    "                              interactive=True)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "\n",
    "    timer = gr.Timer(value=5, active=True)\n",
    "    timer.tick(get_html_logs, inputs=None, outputs=[logs_box])\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        logging.info(f\"User message: {message}\")\n",
    "        yield \"\", history, get_html_logs()\n",
    "        \n",
    "    def set_initial_prompt():\n",
    "        return [{\"role\": \"assistant\", \"content\": initial_prompt}]\n",
    "    \n",
    "    ui.load(set_initial_prompt, inputs=None, outputs=chatbot)\n",
    "    \n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot, logs_box]).then(\n",
    "        chat, inputs=[chatbot, Model], outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    clear.click(clear_logs, inputs=None, outputs=[chatbot, logs_box], queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

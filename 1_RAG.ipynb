{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c029e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf83ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import gradio as gr\n",
    "from io import StringIO\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccbd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"agile_process\"\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8345ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58e04c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 17\n",
      "Document types found: {'Process_Doc'}\n"
     ]
    }
   ],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Embed a single query text.\"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()\n",
    "\n",
    "def load_docx_content(file_path):\n",
    "    \"\"\"Load content from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        content = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                content.append(paragraph.text.strip())\n",
    "        return '\\n'.join(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Simple text splitter.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - chunk_overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Load all DOCX files from the Documents folder\n",
    "files = glob.glob(\"Documents/*.docx\")\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "for file_path in files:\n",
    "    doc_type = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    content = load_docx_content(file_path)\n",
    "    \n",
    "    if content:\n",
    "        chunks = split_text(content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            all_metadatas.append({\n",
    "                \"doc_type\": doc_type,\n",
    "                \"file_path\": file_path,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "            all_ids.append(f\"{doc_type}_{i}_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "print(f\"Total number of chunks: {len(all_chunks)}\")\n",
    "print(f\"Document types found: {set(meta['doc_type'] for meta in all_metadatas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982bbc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing collection deleted.\n",
      "Added batch 1/1\n",
      "Vectorstore created with 17 documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"agile_process\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "collection_name = \"process_docs\"\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(\"Existing collection deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection didn't exist or couldn't be deleted: {e}\")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Add documents to the collection in batches (ChromaDB has limits)\n",
    "batch_size = 100\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    batch_chunks = all_chunks[i:i + batch_size]\n",
    "    batch_metadatas = all_metadatas[i:i + batch_size]\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    \n",
    "    # Generate embeddings for this batch\n",
    "    batch_embeddings = embeddings_model.embed_documents(batch_chunks)\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=batch_chunks,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_chunks) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "# Example: Query the collection\n",
    "def query_documents(query_text, n_results=5):\n",
    "    \"\"\"Query the document collection.\"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query_text)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test query (uncomment to test)\n",
    "# results = query_documents(\"your query here\")\n",
    "# print(\"Query results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b16917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLLogFormatter(logging.Formatter):\n",
    "    AGENT_COLORS = {\n",
    "        \"FrontierAgent\": \"blue\",\n",
    "        \"Default\": \"black\"\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        record.asctime = self.formatTime(record, self.datefmt)\n",
    "        logger_name = record.name.split('.')[-1]  # often \"__main__\" or module\n",
    "        color = self.AGENT_COLORS.get(logger_name, self.AGENT_COLORS[\"Default\"])\n",
    "        return f'<div style=\"color:{color}\">[{record.asctime}] [{logger_name}] [{record.levelname}] {record.getMessage()}</div>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4812ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_stream = StringIO()\n",
    "html_log_stream = StringIO()\n",
    "\n",
    "def init_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    \n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Memory buffer handler\n",
    "    stream_handler = logging.StreamHandler(log_stream)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s] [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "     # HTML handler for Gradio\n",
    "    html_stream_handler = logging.StreamHandler(html_log_stream)\n",
    "    html_stream_handler.setLevel(logging.INFO)\n",
    "    html_stream_handler.setFormatter(HTMLLogFormatter(\n",
    "        fmt=\"%(asctime)s [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\"\n",
    "    ))\n",
    "    root.addHandler(html_stream_handler)\n",
    "\n",
    "    root.addHandler(handler)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e79d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_logs():\n",
    "    html_log_stream.seek(0)\n",
    "    return html_log_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb344bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_logs():\n",
    "    log_stream.truncate(0)\n",
    "    log_stream.seek(0)\n",
    "    html_log_stream.truncate(0)\n",
    "    html_log_stream.seek(0)\n",
    "    return None, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e281990",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6960e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You devise the strategy on how to set up and start a new data engineering project from scratch. You ask the user\n",
    "    to share details about source systems, the number of final data marts, and a brief outline of the overall architecture. Do not proceed \n",
    "    unless the user shares the information related to overall architecture, details about source systems, and the number of target data\n",
    "    marts. On getting the aforesaid information from the user, you will detail out the epics, features, and user stories needed to \n",
    "    achieve the end objective, which is to create the final data marts.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3bbedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_context_function = {\n",
    "    \"name\": \"return_context\",\n",
    "    \"description\": \"\"\"Call this tool after the user confirms the details about source systems, the number of target data marts, and the \n",
    "    overall architecture. Please share \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"details_source_systems\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The details about source systems\",\n",
    "            },\n",
    "            \"overall_architecture\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the architecture\",\n",
    "            },\n",
    "            \"target_data_marts\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the target data marts\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"details_source_systems\",\"overall_architecture\",\"target_data_marts\"],\n",
    "    \"additionalProperties\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff50278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": return_context_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bceaf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(name, args):\n",
    "    source = args.get('details_source_systems')\n",
    "    architecture = args.get('overall_architecture')\n",
    "    marts = args.get('target_data_marts')\n",
    "    if name.replace('\"','') == \"return_context\":\n",
    "        context=return_context(collection, f\"Source details -\\n{source}\\n\\nArchitecture Details -\\n{architecture}\\n\\nMart Details -\\n{marts}\")\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a59f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_context(collection, user_query):\n",
    "    context = \"\\n\\nProviding some context from relevant information -\\n\\n\"\n",
    "    retrieved = collection.query(\n",
    "        query_embeddings=[embeddings_model.embed_query(user_query)],\n",
    "        n_results=10,  # e.g., 5 or 10\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    retrieved_chunks = retrieved[\"documents\"][0]\n",
    "    context+= \"\\n\\n\".join(retrieved_chunks)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602012dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_open_ai(history):\n",
    "    openai=OpenAI()\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history \n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages, tools=tools)\n",
    "\n",
    "    tool_responses = []\n",
    "\n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_calls = message.tool_calls  # renamed to avoid UnboundLocalError\n",
    "\n",
    "        print(f\"tool calls \\n\\n {tool_calls}\")\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_id = tool_call.id\n",
    "            name = tool_call.function.name\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            # Call the tool handler\n",
    "            result = \"\"\n",
    "            if name == \"return_context\":\n",
    "                result = handle_tool_call(name, args)\n",
    "\n",
    "            tool_responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_id,\n",
    "                \"content\": json.dumps(result),\n",
    "            })\n",
    "\n",
    "        print(f\"tool responses {tool_responses}\")\n",
    "        messages.append(message)\n",
    "        messages.extend(tool_responses)  # important fix here\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0d03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(Model, history):\n",
    "    if Model==\"Open AI\":\n",
    "        openai = OpenAI()\n",
    "        history = chat_open_ai(history)\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299ac729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "[2025-06-16 19:49:30 +0530] [Agents] [INFO] HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "[2025-06-16 19:49:30 +0530] [Agents] [INFO] HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-16 19:49:30 +0530] [Agents] [INFO] HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2025-06-16 19:49:41 +0530] [Agents] [INFO] User message: hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\blocks.py\", line 2230, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\blocks.py\", line 2012, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\components\\chatbot.py\", line 633, in postprocess\n",
      "    self._check_format(value, \"messages\")\n",
      "  File \"c:\\Users\\aruna\\Desktop\\Practice\\Data_Science\\LLM\\Advanced\\Projects\\Business Games\\kbg\\lib\\site-packages\\gradio\\components\\chatbot.py\", line 423, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css=\"\"\"\n",
    "    #log_box {\n",
    "        height: 300px;\n",
    "        overflow-y: scroll;\n",
    "        border: 1px solid #ccc;\n",
    "        padding: 10px;\n",
    "        background: #f9f9f9;\n",
    "        font-family: monospace;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\"\"\") as ui:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=300, type=\"messages\")\n",
    "        with gr.Column(scale=1):\n",
    "            \n",
    "            logs_box = gr.HTML(label=\"Logs\", elem_id=\"log_box\")\n",
    "    with gr.Row():\n",
    "        Model = gr.Dropdown([\"Open AI\",\"XX\"],\n",
    "                              # value=[\"Open AI\",\"Claude\"],\n",
    "                              multiselect=False,\n",
    "                              label=\"Model\",\n",
    "                              interactive=True)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "\n",
    "    timer = gr.Timer(value=5, active=True)\n",
    "    timer.tick(get_html_logs, inputs=None, outputs=[logs_box])\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        logging.info(f\"User message: {message}\")\n",
    "        yield \"\", history, get_html_logs()\n",
    "        \n",
    "    \n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot, logs_box]).then(\n",
    "        chat, inputs=[chatbot, Model], outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    clear.click(clear_logs, inputs=None, outputs=[chatbot, logs_box], queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5df27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c029e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf83ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import gradio as gr\n",
    "from io import StringIO\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccbd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"agile_process\"\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8345ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58486cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Embed a single query text.\"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58e04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_docx_content(file_path):\n",
    "#     \"\"\"Load content from a DOCX file.\"\"\"\n",
    "#     try:\n",
    "#         doc = Document(file_path)\n",
    "#         content = []\n",
    "#         for paragraph in doc.paragraphs:\n",
    "#             if paragraph.text.strip():\n",
    "#                 content.append(paragraph.text.strip())\n",
    "#         return '\\n'.join(content)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def split_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "#     \"\"\"Simple text splitter.\"\"\"\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     while start < len(text):\n",
    "#         end = start + chunk_size\n",
    "#         chunk = text[start:end]\n",
    "#         chunks.append(chunk)\n",
    "#         start = end - chunk_overlap\n",
    "#         if start >= len(text):\n",
    "#             break\n",
    "#     return chunks\n",
    "\n",
    "# # Load all DOCX files from the Documents folder\n",
    "# files = glob.glob(\"Documents/*.docx\")\n",
    "# all_chunks = []\n",
    "# all_metadatas = []\n",
    "# all_ids = []\n",
    "\n",
    "# for file_path in files:\n",
    "#     doc_type = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     content = load_docx_content(file_path)\n",
    "    \n",
    "#     if content:\n",
    "#         chunks = split_text(content)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             all_chunks.append(chunk)\n",
    "#             all_metadatas.append({\n",
    "#                 \"doc_type\": doc_type,\n",
    "#                 \"file_path\": file_path,\n",
    "#                 \"chunk_index\": i\n",
    "#             })\n",
    "#             all_ids.append(f\"{doc_type}_{i}_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "# print(f\"Total number of chunks: {len(all_chunks)}\")\n",
    "# print(f\"Document types found: {set(meta['doc_type'] for meta in all_metadatas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34e97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"agile_process\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n",
    "collection_name = \"process_docs\"\n",
    "collection = client.get_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982bbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to create the vectorstore again\n",
    "\n",
    "# # Delete existing collection if it exists\n",
    "# collection_name = \"process_docs\"\n",
    "# try:\n",
    "#     client.delete_collection(name=collection_name)\n",
    "#     print(\"Existing collection deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Collection didn't exist or couldn't be deleted: {e}\")\n",
    "\n",
    "# # Create a new collection\n",
    "# collection = client.create_collection(\n",
    "#     name=collection_name,\n",
    "#     metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "# )\n",
    "\n",
    "# # Add documents to the collection in batches (ChromaDB has limits)\n",
    "# batch_size = 100\n",
    "# for i in range(0, len(all_chunks), batch_size):\n",
    "#     batch_chunks = all_chunks[i:i + batch_size]\n",
    "#     batch_metadatas = all_metadatas[i:i + batch_size]\n",
    "#     batch_ids = all_ids[i:i + batch_size]\n",
    "    \n",
    "#     # Generate embeddings for this batch\n",
    "#     batch_embeddings = embeddings_model.embed_documents(batch_chunks)\n",
    "    \n",
    "#     # Add to collection\n",
    "#     collection.add(\n",
    "#         documents=batch_chunks,\n",
    "#         metadatas=batch_metadatas,\n",
    "#         ids=batch_ids,\n",
    "#         embeddings=batch_embeddings\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Added batch {i//batch_size + 1}/{(len(all_chunks) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "# print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "# # Test query (uncomment to test)\n",
    "# # results = query_documents(\"your query here\")\n",
    "# # print(\"Query results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d3a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query the collection\n",
    "def query_documents(query_text, n_results=5):\n",
    "    \"\"\"Query the document collection.\"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query_text)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLLogFormatter(logging.Formatter):\n",
    "    AGENT_COLORS = {\n",
    "        \"FrontierAgent\": \"blue\",\n",
    "        \"Default\": \"black\"\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        record.asctime = self.formatTime(record, self.datefmt)\n",
    "        logger_name = record.name.split('.')[-1]  # often \"__main__\" or module\n",
    "        color = self.AGENT_COLORS.get(logger_name, self.AGENT_COLORS[\"Default\"])\n",
    "        return f'<div style=\"color:{color}\">[{record.asctime}] [{logger_name}] [{record.levelname}] {record.getMessage()}</div>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4812ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_stream = StringIO()\n",
    "html_log_stream = StringIO()\n",
    "\n",
    "def init_logging():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    \n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Memory buffer handler\n",
    "    stream_handler = logging.StreamHandler(log_stream)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s] [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "     # HTML handler for Gradio\n",
    "    html_stream_handler = logging.StreamHandler(html_log_stream)\n",
    "    html_stream_handler.setLevel(logging.INFO)\n",
    "    html_stream_handler.setFormatter(HTMLLogFormatter(\n",
    "        fmt=\"%(asctime)s [Agents] [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S %z\"\n",
    "    ))\n",
    "    root.addHandler(html_stream_handler)\n",
    "\n",
    "    root.addHandler(handler)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e79d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_logs():\n",
    "    html_log_stream.seek(0)\n",
    "    return html_log_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb344bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_logs():\n",
    "    log_stream.truncate(0)\n",
    "    log_stream.seek(0)\n",
    "    html_log_stream.truncate(0)\n",
    "    html_log_stream.seek(0)\n",
    "    return None, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e281990",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6960e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You devise the strategy on how to set up and start a new data engineering project from scratch. You ask the user\n",
    "    to share details about source systems, the number of final data marts, and a brief outline of the overall architecture. Do not proceed \n",
    "    unless the user shares the information related to overall architecture, details about source systems, and the number of target data\n",
    "    marts. On getting the aforesaid information from the user, you will make the tool call using the return_context_function and strictly\n",
    "    adhere to the agile process details that the tool call returns. You will eventually display the epics, features, and user stories \n",
    "    needed to achieve the end objective, which is to create the final data marts. Always provide epic, feature, and story details and \n",
    "    include points for each story along with the output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3bbedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_context_function = {\n",
    "    \"name\": \"return_context\",\n",
    "    \"description\": \"\"\"Call this tool after the user confirms the details about source systems, the number of target data marts, and the \n",
    "    overall architecture. For any details related to agile processes, you will strictly adhere to the output that you gather from this \n",
    "    tool call.\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"details_source_systems\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The details about source systems\",\n",
    "            },\n",
    "            \"overall_architecture\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the architecture\",\n",
    "            },\n",
    "            \"target_data_marts\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Details about the target data marts\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"details_source_systems\",\"overall_architecture\",\"target_data_marts\"],\n",
    "    \"additionalProperties\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff50278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": return_context_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bceaf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(name, args):\n",
    "    source = args.get('details_source_systems')\n",
    "    architecture = args.get('overall_architecture')\n",
    "    marts = args.get('target_data_marts')\n",
    "    if name.replace('\"','') == \"return_context\":\n",
    "        context=return_context(collection, f\"Source details -\\n{source}\\n\\nArchitecture Details -\\n{architecture}\\n\\nMart Details -\\n{marts}\")\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a59f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_context(collection, user_query):\n",
    "    context = \"\\n\\nProviding some context from relevant information -\\n\\n\"\n",
    "    retrieved = collection.query(\n",
    "        query_embeddings=[embeddings_model.embed_query(user_query)],\n",
    "        n_results=10,  # e.g., 5 or 10\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    retrieved_chunks = retrieved[\"documents\"][0]\n",
    "    context+= \"\\n\\n\".join(retrieved_chunks)\n",
    "    print(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602012dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_open_ai(history):\n",
    "    openai=OpenAI()\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history \n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages, tools=tools)\n",
    "\n",
    "    tool_responses = []\n",
    "\n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_calls = message.tool_calls  # renamed to avoid UnboundLocalError\n",
    "\n",
    "        print(f\"tool calls \\n\\n {tool_calls}\")\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_id = tool_call.id\n",
    "            name = tool_call.function.name\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            # Call the tool handler\n",
    "            result = \"\"\n",
    "            if name == \"return_context\":\n",
    "                result = handle_tool_call(name, args)\n",
    "\n",
    "            tool_responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_id,\n",
    "                \"content\": json.dumps(result),\n",
    "            })\n",
    "\n",
    "        print(f\"tool responses {tool_responses}\")\n",
    "        messages.append(message)\n",
    "        messages.extend(tool_responses)  # important fix here\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b0d03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history,Model):\n",
    "    if Model==\"Open AI\":\n",
    "        openai = OpenAI()\n",
    "        history = chat_open_ai(history)\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299ac729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "[2025-06-17 13:20:01 +0530] [Agents] [INFO] HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "[2025-06-17 13:20:01 +0530] [Agents] [INFO] HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-17 13:20:02 +0530] [Agents] [INFO] HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2025-06-17 13:20:11 +0530] [Agents] [INFO] User message: The source system comprises of 2 IBM DB2 Tables\n",
      "The final outcome are 2 data marts - one for account valuations details and the other for transactions summary\n",
      "The architecture is as follows - From the IBM DB2 source tables, data is being transferred from the two source tables leveraging Airflow DAGs and IBM CDC subscriptions. Post landing the data at Snowflake Landing layer, data is transformed and populated at the final data marts at the final \"gold layer\" in snowflake, for both account valuations details and transactions summary\n",
      "[2025-06-17 13:20:13 +0530] [Agents] [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "tool calls \n",
      "\n",
      " [ChatCompletionMessageToolCall(id='call_wPU0tjmd5JoZaZVPtHKeg7AD', function=Function(arguments='{\"details_source_systems\":\"2 IBM DB2 Tables\",\"overall_architecture\":\"From the IBM DB2 source tables, data is being transferred from the two source tables leveraging Airflow DAGs and IBM CDC subscriptions. Post landing the data at Snowflake Landing layer, data is transformed and populated at the final data marts at the final \\'gold layer\\' in Snowflake, for both account valuations details and transactions summary.\",\"target_data_marts\":\"2 data marts - one for account valuations details and the other for transactions summary\"}', name='return_context'), type='function')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403f8e5dde2a4afe88ec9dce663d1449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Providing some context from relevant information -\n",
      "\n",
      "ake Landing Zone (21-40)-8 Story points – Ownership with Consultants (Developers)\n",
      "Testing (Manual/Automated) to check data consistency across Hadoop and AWS S3 – 8 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 5\n",
      "Loading tables to Snowflake Landing Zone (41-50)-5 Story points – Ownership with Consultants (Developers)\n",
      "Feature – Commence Mapping from landing to Intermediate Staging Layer and Populate Staging Tables – Size - M\n",
      "Sprint 5 –\n",
      "Commence Mapping sheet updates from Landing to Staging Layer – 8 Story Points – Ownership – Consultants (Business Analysts) and Client\n",
      "POC - Staging table creation at Snowflake Staging layer – 8 Story Points – Ownership – Client and Consultants (Developers)\n",
      "Testing (Manual/Automated) to check data consistency across Hadoop and S3 – 5 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Testing (Manual/Automated) to check data consistency across Snowflake Landing Zone and Snowflake Staging Layer – 3 Story Points – Ownership – Consultants (QA E\n",
      "\n",
      "itects)\n",
      "Load the data to AWS S3 tables (1-10) – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Sprint 3\n",
      "Load data to AWS S3 tables (11-30) – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Load data to AWS S3 tables (31-50) – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Feature - Loading Source Tables (50) from AWS S3 to Snowflake– Comprising Specific Tech Stories such as – Size -M\n",
      "Sprint 3\n",
      "POC on S3 to Snowflake Connectivity – 8 Story Points- Joint Ownership- Client and Consultants (Developers and Architect)\n",
      "Sprint 4\n",
      "Finalizing S3 to Snowflake Connectivity - 5 Story Points- Joint Ownership- Client and Consultants (Developers and Architect)\n",
      "Loading tables to Snowflake Landing Zone (1-20)-8 Story points – Ownership with Consultants (Developers)\n",
      "Loading tables to Snowflake Landing Zone (21-40)-8 Story points – Ownership with Consultants (Developers)\n",
      "Testing (Manual/Automated) to check data consistency across Hadoop and AWS S3 – 8 Story Points – Ownership – Consultan\n",
      "\n",
      " – Ownership – Consultants (QA Engineer)\n",
      "Testing (Manual/Automated) to check data consistency across Snowflake Landing Zone and Snowflake Staging Layer – 3 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 6 –\n",
      "Finalizing Mapping Sheet from Landing to Staging Layer – 5 Story Points – Ownership – Consultants (Business Analysts) and Client\n",
      "Populating Staging tables with data (tables 1-20)– 8 Story points – Ownership – Consultants (Developers)\n",
      "Populating Staging tables with data (tables 21-40)– 8 Story points – Ownership – Consultants (Developers)\n",
      "Testing (Manual/Automated) to check data consistency across Snowflake Landing and Snowflake Staging – 8 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 7 –\n",
      "Populating Staging tables with data (tables 41-50)– 5 Story points – Ownership – Consultants (Developers)\n",
      "Feature - Commence Mapping from Staging to Data Mart and Populate Final Mart Tables – Size - L\n",
      "Sprint 7 –\n",
      "Commence Mapping from Staging Area to Final Mart (Gold Layer\n",
      "\n",
      ".\n",
      "For fixing any bug, all the above steps will be applicable with the only difference that the first story will be Bug Analysis (1 story point max) instead of Report Analysis.\n",
      "Story Pointing Guidelines\n",
      "Sprint Length is assumed to be 2 weeks- 10 working days\n",
      "Maximum Story points per story in a sprint – 8\n",
      "How to Structure?\n",
      "Example Usage – Let’s have the following problem statement –\n",
      "There is data residing in legacy systems such as Hadoop.\n",
      "The architecture expects the data to be pushed to AWS S3, and later to Snowflake.\n",
      "In total, data from 50 source tables are needed to be uploaded to S3 and subsequently to Snowflake.\n",
      "Later, from the source tables at S3, data is loaded to Snowflake (called Landing stage).\n",
      "Data Marts are required to be populated for end-consumption post transformations from landing to staging layer (intermediate) and then from staging to Gold Layer (ay) with the final marts at Gold Layer. For this example, let us consider only one Data mart needs to be created.\n",
      "To conceptu\n",
      "\n",
      "ership – Consultants (Developers)\n",
      "Feature - Commence Mapping from Staging to Data Mart and Populate Final Mart Tables – Size - L\n",
      "Sprint 7 –\n",
      "Commence Mapping from Staging Area to Final Mart (Gold Layer) – 8 Story Points – Ownership- Client and Consultants (Business Analysts)\n",
      "POC – Create tables on Final Gold Layer – 8 Story Points – Ownership – Client and Consultants (Developers)\n",
      "Testing (Manual/Automated) to check data consistency across Snowflake Landing and Snowflake Staging – 3 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 8 –\n",
      "Concluding Mapping from Staging Area to Final mart (Gold Layer) – 8 Story Points - Ownership- Client and Consultants (Business Analysts)\n",
      "Finalize – Creation of tables on Final Gold Layer – 8 Story Points – Ownership – Client and Consultants (Developers)\n",
      "Populate Final Mart Tables at Gold Layer – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Testing- Functional Understanding and Manual Testing – 3 Story Points – Ownership – Consultants (QA\n",
      "\n",
      "bove Problem Statement, the following is likely to be the Epics/ Feature/ Story Structure –\n",
      "Epic – Quarter XX\n",
      "Feature- Loading Source Tables (50) from Hadoop to AWS S3– Comprising Specific Tech Stories such as – Size - L\n",
      "Sprint 1\n",
      "Team Onboarding and Accesses – 8 Story Points – Ownership with Client/ Business Vertical – Admin and Sales\n",
      "Architecture Review – 8 Story Points – Joint Ownership- Client and Consultants – Consultants (Architects) and Client (Architects)\n",
      "POC on Connectivity (Hadoop to AWS S3) - 8 Story Points - Joint Ownership- Client and Consultants (Developers)\n",
      "Sprint 2\n",
      "Finalizing Onboarding and Accesses – 8 Story points - Ownership with Client/ Business Vertical- Admin and Sales\n",
      "Finalizing Architecture - 8 Story Points – Joint Ownership- Client (Architects) and Consultants (Architects)\n",
      "Load the data to AWS S3 tables (1-10) – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Sprint 3\n",
      "Load data to AWS S3 tables (11-30) – 8 Story Points – Ownership – Consultants (Developers\n",
      "\n",
      "nding to staging layer (intermediate) and then from staging to Gold Layer (ay) with the final marts at Gold Layer. For this example, let us consider only one Data mart needs to be created.\n",
      "To conceptualize the data marts, Source (Landing Layer) to Target (Gold Layer) Mapping sheets need to be devised by the Business Analysts in discussion with the Data Product Owners and other Subject Matter Experts.\n",
      "Post the mapping sheets are ready, Developers start the coding work and migrate the code to UAT.\n",
      "Test Engineers run automated Regression Suits to test the code.\n",
      "After development of all the features, the code is finally productionized\n",
      "We assumed there is only one environment (Production). In practice, All work Sprint 3 onwards will be repeated for Dev, UAT and Production environments.\n",
      "On the above Problem Statement, the following is likely to be the Epics/ Feature/ Story Structure –\n",
      "Epic – Quarter XX\n",
      "Feature- Loading Source Tables (50) from Hadoop to AWS S3– Comprising Specific Tech Storie\n",
      "\n",
      "rs)\n",
      "Populate Final Mart Tables at Gold Layer – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Testing- Functional Understanding and Manual Testing – 3 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 9\n",
      "Populate Final Mart Tables at Gold Layer – 8 Story Points – Ownership – Consultants (Developers)\n",
      "Unit Testing - 8 Story Points – Ownership – Consultants (Developers)\n",
      "Testing- Functional Understanding and Manual Testing – 5 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Testing- Creating Regression Scenarios – 3 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Sprint 10\n",
      "Testing- Completing creation of Regression Scenarios – 8 Story Points – Ownership – Consultants (QA Engineer)\n",
      "Review – 8 Story Points – Ownership – Consultants (Business Analysts)\n",
      "Sprint 11\n",
      "Testing – Running Regression Scenarios and submitting Evidence – 8 Story Points – Ownership – Consultants (QA Engineer)\n",
      "PO Review (Product Owner)\n",
      "\n",
      "e review is complete\n",
      "ATF testing is passed\n",
      "Regression testing has completed\n",
      "Acceptance criteria have been met\n",
      "Signed off by the PO\n",
      "Confluence updated as required\n",
      "Ready to demo the progress\n",
      "Workflow – For implementing any Feature, the following are the standard stories that will be taken up by the development team (including Business Analysts, Architects, Technical Leads, Developers, and Test Engineers)\n",
      "Report Analysis – Analyzing existing reports from legacy systems to provide feedbacks or any inputs during the 2nd step of preparing or updating the mapping document. Output – Feedback for mapping document preparation.\n",
      "Requirement Gathering/ Data Mapping & Updates – based on the feedback received during the Requirement analysis phases, the Business Analysts will design the mapping sheet (which in turn is needed by the developers to start the build phase). Output – Mapping Sheet approved by the Product Owner and ready for development work to begin.\n",
      "In case of pure technical stories where \n",
      "\n",
      "Agile Process Document – How to start a Data Engineering Project\n",
      "Definitions and Purpose\n",
      "Item – Initiative\n",
      "Owner- Predefined\n",
      "Purpose –\n",
      "High-level vision covering the desired and expected business goals for their delivery\n",
      "Decision made to create an initiative for each quarter of the year. So there will be 4 Initiatives per calendar year.\n",
      "Sign-off – Initiative closes once related epics are completed and signed-off or once the quarter gets over, whichever is earlier. In the latter case, the incomplete epics get spilled over to the next initiative.\n",
      "Item – Epic\n",
      "Owner – Product Owner (PO)\n",
      "Purpose –\n",
      "High-level definition of the workstream or business outcome delivering some value\n",
      "Product Owner to define the acceptance criteria at the epic level\n",
      "Sign-off – Epic closes once related features are completed and signed-off or once the quarter gets over, whichever is earlier. In the latter case, the incomplete features get spilled over to a similar epic next quarter.\n",
      "Item- Feature\n",
      "Owner – Product Ow\n",
      "tool responses [{'role': 'tool', 'tool_call_id': 'call_wPU0tjmd5JoZaZVPtHKeg7AD', 'content': '\"\\\\n\\\\nProviding some context from relevant information -\\\\n\\\\nake Landing Zone (21-40)-8 Story points \\\\u2013 Ownership with Consultants (Developers)\\\\nTesting (Manual/Automated) to check data consistency across Hadoop and AWS S3 \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 5\\\\nLoading tables to Snowflake Landing Zone (41-50)-5 Story points \\\\u2013 Ownership with Consultants (Developers)\\\\nFeature \\\\u2013 Commence Mapping from landing to Intermediate Staging Layer and Populate Staging Tables \\\\u2013 Size - M\\\\nSprint 5 \\\\u2013\\\\nCommence Mapping sheet updates from Landing to Staging Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Business Analysts) and Client\\\\nPOC - Staging table creation at Snowflake Staging layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Client and Consultants (Developers)\\\\nTesting (Manual/Automated) to check data consistency across Hadoop and S3 \\\\u2013 5 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nTesting (Manual/Automated) to check data consistency across Snowflake Landing Zone and Snowflake Staging Layer \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA E\\\\n\\\\nitects)\\\\nLoad the data to AWS S3 tables (1-10) \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nSprint 3\\\\nLoad data to AWS S3 tables (11-30) \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nLoad data to AWS S3 tables (31-50) \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nFeature - Loading Source Tables (50) from AWS S3 to Snowflake\\\\u2013 Comprising Specific Tech Stories such as \\\\u2013 Size -M\\\\nSprint 3\\\\nPOC on S3 to Snowflake Connectivity \\\\u2013 8 Story Points- Joint Ownership- Client and Consultants (Developers and Architect)\\\\nSprint 4\\\\nFinalizing S3 to Snowflake Connectivity - 5 Story Points- Joint Ownership- Client and Consultants (Developers and Architect)\\\\nLoading tables to Snowflake Landing Zone (1-20)-8 Story points \\\\u2013 Ownership with Consultants (Developers)\\\\nLoading tables to Snowflake Landing Zone (21-40)-8 Story points \\\\u2013 Ownership with Consultants (Developers)\\\\nTesting (Manual/Automated) to check data consistency across Hadoop and AWS S3 \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultan\\\\n\\\\n \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nTesting (Manual/Automated) to check data consistency across Snowflake Landing Zone and Snowflake Staging Layer \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 6 \\\\u2013\\\\nFinalizing Mapping Sheet from Landing to Staging Layer \\\\u2013 5 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Business Analysts) and Client\\\\nPopulating Staging tables with data (tables 1-20)\\\\u2013 8 Story points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nPopulating Staging tables with data (tables 21-40)\\\\u2013 8 Story points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nTesting (Manual/Automated) to check data consistency across Snowflake Landing and Snowflake Staging \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 7 \\\\u2013\\\\nPopulating Staging tables with data (tables 41-50)\\\\u2013 5 Story points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nFeature - Commence Mapping from Staging to Data Mart and Populate Final Mart Tables \\\\u2013 Size - L\\\\nSprint 7 \\\\u2013\\\\nCommence Mapping from Staging Area to Final Mart (Gold Layer\\\\n\\\\n.\\\\nFor fixing any bug, all the above steps will be applicable with the only difference that the first story will be Bug Analysis (1 story point max) instead of Report Analysis.\\\\nStory Pointing Guidelines\\\\nSprint Length is assumed to be 2 weeks- 10 working days\\\\nMaximum Story points per story in a sprint \\\\u2013 8\\\\nHow to Structure?\\\\nExample Usage \\\\u2013 Let\\\\u2019s have the following problem statement \\\\u2013\\\\nThere is data residing in legacy systems such as Hadoop.\\\\nThe architecture expects the data to be pushed to AWS S3, and later to Snowflake.\\\\nIn total, data from 50 source tables are needed to be uploaded to S3 and subsequently to Snowflake.\\\\nLater, from the source tables at S3, data is loaded to Snowflake (called Landing stage).\\\\nData Marts are required to be populated for end-consumption post transformations from landing to staging layer (intermediate) and then from staging to Gold Layer (ay) with the final marts at Gold Layer. For this example, let us consider only one Data mart needs to be created.\\\\nTo conceptu\\\\n\\\\nership \\\\u2013 Consultants (Developers)\\\\nFeature - Commence Mapping from Staging to Data Mart and Populate Final Mart Tables \\\\u2013 Size - L\\\\nSprint 7 \\\\u2013\\\\nCommence Mapping from Staging Area to Final Mart (Gold Layer) \\\\u2013 8 Story Points \\\\u2013 Ownership- Client and Consultants (Business Analysts)\\\\nPOC \\\\u2013 Create tables on Final Gold Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Client and Consultants (Developers)\\\\nTesting (Manual/Automated) to check data consistency across Snowflake Landing and Snowflake Staging \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 8 \\\\u2013\\\\nConcluding Mapping from Staging Area to Final mart (Gold Layer) \\\\u2013 8 Story Points - Ownership- Client and Consultants (Business Analysts)\\\\nFinalize \\\\u2013 Creation of tables on Final Gold Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Client and Consultants (Developers)\\\\nPopulate Final Mart Tables at Gold Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nTesting- Functional Understanding and Manual Testing \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA\\\\n\\\\nbove Problem Statement, the following is likely to be the Epics/ Feature/ Story Structure \\\\u2013\\\\nEpic \\\\u2013 Quarter XX\\\\nFeature- Loading Source Tables (50) from Hadoop to AWS S3\\\\u2013 Comprising Specific Tech Stories such as \\\\u2013 Size - L\\\\nSprint 1\\\\nTeam Onboarding and Accesses \\\\u2013 8 Story Points \\\\u2013 Ownership with Client/ Business Vertical \\\\u2013 Admin and Sales\\\\nArchitecture Review \\\\u2013 8 Story Points \\\\u2013 Joint Ownership- Client and Consultants \\\\u2013 Consultants (Architects) and Client (Architects)\\\\nPOC on Connectivity (Hadoop to AWS S3) - 8 Story Points - Joint Ownership- Client and Consultants (Developers)\\\\nSprint 2\\\\nFinalizing Onboarding and Accesses \\\\u2013 8 Story points - Ownership with Client/ Business Vertical- Admin and Sales\\\\nFinalizing Architecture - 8 Story Points \\\\u2013 Joint Ownership- Client (Architects) and Consultants (Architects)\\\\nLoad the data to AWS S3 tables (1-10) \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nSprint 3\\\\nLoad data to AWS S3 tables (11-30) \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers\\\\n\\\\nnding to staging layer (intermediate) and then from staging to Gold Layer (ay) with the final marts at Gold Layer. For this example, let us consider only one Data mart needs to be created.\\\\nTo conceptualize the data marts, Source (Landing Layer) to Target (Gold Layer) Mapping sheets need to be devised by the Business Analysts in discussion with the Data Product Owners and other Subject Matter Experts.\\\\nPost the mapping sheets are ready, Developers start the coding work and migrate the code to UAT.\\\\nTest Engineers run automated Regression Suits to test the code.\\\\nAfter development of all the features, the code is finally productionized\\\\nWe assumed there is only one environment (Production). In practice, All work Sprint 3 onwards will be repeated for Dev, UAT and Production environments.\\\\nOn the above Problem Statement, the following is likely to be the Epics/ Feature/ Story Structure \\\\u2013\\\\nEpic \\\\u2013 Quarter XX\\\\nFeature- Loading Source Tables (50) from Hadoop to AWS S3\\\\u2013 Comprising Specific Tech Storie\\\\n\\\\nrs)\\\\nPopulate Final Mart Tables at Gold Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nTesting- Functional Understanding and Manual Testing \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 9\\\\nPopulate Final Mart Tables at Gold Layer \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nUnit Testing - 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Developers)\\\\nTesting- Functional Understanding and Manual Testing \\\\u2013 5 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nTesting- Creating Regression Scenarios \\\\u2013 3 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nSprint 10\\\\nTesting- Completing creation of Regression Scenarios \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nReview \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (Business Analysts)\\\\nSprint 11\\\\nTesting \\\\u2013 Running Regression Scenarios and submitting Evidence \\\\u2013 8 Story Points \\\\u2013 Ownership \\\\u2013 Consultants (QA Engineer)\\\\nPO Review (Product Owner)\\\\n\\\\ne review is complete\\\\nATF testing is passed\\\\nRegression testing has completed\\\\nAcceptance criteria have been met\\\\nSigned off by the PO\\\\nConfluence updated as required\\\\nReady to demo the progress\\\\nWorkflow \\\\u2013 For implementing any Feature, the following are the standard stories that will be taken up by the development team (including Business Analysts, Architects, Technical Leads, Developers, and Test Engineers)\\\\nReport Analysis \\\\u2013 Analyzing existing reports from legacy systems to provide feedbacks or any inputs during the 2nd step of preparing or updating the mapping document. Output \\\\u2013 Feedback for mapping document preparation.\\\\nRequirement Gathering/ Data Mapping & Updates \\\\u2013 based on the feedback received during the Requirement analysis phases, the Business Analysts will design the mapping sheet (which in turn is needed by the developers to start the build phase). Output \\\\u2013 Mapping Sheet approved by the Product Owner and ready for development work to begin.\\\\nIn case of pure technical stories where \\\\n\\\\nAgile Process Document \\\\u2013 How to start a Data Engineering Project\\\\nDefinitions and Purpose\\\\nItem \\\\u2013 Initiative\\\\nOwner- Predefined\\\\nPurpose \\\\u2013\\\\nHigh-level vision covering the desired and expected business goals for their delivery\\\\nDecision made to create an initiative for each quarter of the year. So there will be 4 Initiatives per calendar year.\\\\nSign-off \\\\u2013 Initiative closes once related epics are completed and signed-off or once the quarter gets over, whichever is earlier. In the latter case, the incomplete epics get spilled over to the next initiative.\\\\nItem \\\\u2013 Epic\\\\nOwner \\\\u2013 Product Owner (PO)\\\\nPurpose \\\\u2013\\\\nHigh-level definition of the workstream or business outcome delivering some value\\\\nProduct Owner to define the acceptance criteria at the epic level\\\\nSign-off \\\\u2013 Epic closes once related features are completed and signed-off or once the quarter gets over, whichever is earlier. In the latter case, the incomplete features get spilled over to a similar epic next quarter.\\\\nItem- Feature\\\\nOwner \\\\u2013 Product Ow\"'}]\n",
      "[2025-06-17 13:20:24 +0530] [Agents] [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "initial_prompt = \"\"\"👋 Hello! How can I assist you today? If you're looking to set up a new data engineering project, please \n",
    "provide me with details about the source systems, the number of target data marts, and a brief outline of the overall architecture.\"\"\"\n",
    "\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    #log_box {\n",
    "        height: 300px;\n",
    "        overflow-y: scroll;\n",
    "        border: 1px solid #ccc;\n",
    "        padding: 10px;\n",
    "        background: #f9f9f9;\n",
    "        font-family: monospace;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\"\"\") as ui:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=300, type=\"messages\")\n",
    "        with gr.Column(scale=1):\n",
    "            \n",
    "            logs_box = gr.HTML(label=\"Logs\", elem_id=\"log_box\")\n",
    "    with gr.Row():\n",
    "        Model = gr.Dropdown([\"Open AI\",\"XX\"],\n",
    "                              # value=[\"Open AI\",\"Claude\"],\n",
    "                              multiselect=False,\n",
    "                              label=\"Model\",\n",
    "                              interactive=True)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "\n",
    "    timer = gr.Timer(value=5, active=True)\n",
    "    timer.tick(get_html_logs, inputs=None, outputs=[logs_box])\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        logging.info(f\"User message: {message}\")\n",
    "        yield \"\", history, get_html_logs()\n",
    "        \n",
    "    def set_initial_prompt():\n",
    "        return [{\"role\": \"assistant\", \"content\": initial_prompt}]\n",
    "    \n",
    "    ui.load(set_initial_prompt, inputs=None, outputs=chatbot)\n",
    "    \n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot, logs_box]).then(\n",
    "        chat, inputs=[chatbot, Model], outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    clear.click(clear_logs, inputs=None, outputs=[chatbot, logs_box], queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5df27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009b2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
